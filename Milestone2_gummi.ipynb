{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nDuHs04-5UCU"
   },
   "source": [
    "# Milestone 2 - multiple correlations\n",
    "\n",
    "Find correlations between at least p >= 3 vectors over a threshold t.\n",
    "\n",
    "*   free to choose value of p\n",
    "*   start with p = 3\n",
    "\n",
    "As correlation measures, you should consider the following two:\n",
    "\n",
    "*   total correlation\n",
    "*   pearson correlation, where the vectors of the two sides are computed by linear combinations (e.g. averaged). For example, consider the multiple correlation of vectors representing\n",
    "Rabobank and ING (denoted as R and I, respectively), with the vector representing Microsoft\n",
    "(denoted as M). The multiple correlation is then defined as Corr(M, R+I/2) \n",
    "\n",
    "You also need to define an aggregation function. The functionality of the aggregation function is\n",
    "(possibly) to reduce the number of input time series, e.g., by averaging as in the previous\n",
    "example. Other aggregation functions that might be meaningful include maximum, minimum, the\n",
    "identity function, variance, etc.\n",
    "\n",
    "In particular:\n",
    "- Build and implement an architecture that gets as input your dataset(s), a correlation function,\n",
    "and a suitable aggregation function, and finds the sets with high multiple correlations (over a\n",
    "threshold t). The value of t depends on the dataset and the correlation/aggregation function.\n",
    "Set a value such that you return around 10 results. Your code should not consider solutions\n",
    "where a vector appears more than once in the input parameters (e.g., in both in1 and in2).\n",
    "- Implement the two described correlation functions with two meaningful aggregation functions,\n",
    "and test your code with these. For testing, you can use the same dataset used in Milestone 1,\n",
    "or a non-negligible subset of it.\n",
    "\n",
    "Constraints:\n",
    "1. You should define aggregation and correlation functions separately.\n",
    "2. The aggregation function should implement one of the following generic interfaces (you can\n",
    "slightly modify it if you have a good reason):\n",
    "dataType aggrFunction(List<dataType> in), e.g., for averaging\n",
    "List<dataType> aggrFunction(List<dataType> in), e.g., the identity function\n",
    "3. The correlation function should implement one of the following generic interfaces (you can\n",
    "slightly modify it if you have a good reason):\n",
    "double correlationFunction(aggFunction(List<dataType> in))\n",
    "double correlationFunction(aggFunction(List<dataType> in1),\n",
    "aggFunction(List<dataType> in2))\n",
    "4. The Spark code should take the correlation and aggregation function (any implementations\n",
    "that satisfy the described interface) as input parameters.\n",
    "5. Make sure that you write Spark code that scales out and avoids bottlenecks.\n",
    "Hint: Think how you can efficiently enumerate all possible combinations and assign ranges of 2\n",
    "comparisons to the individual workers.\n",
    "6. When the correlation/aggregation functions are commutative and associative, you should\n",
    "avoid redundant computations. This includes, e.g., computing avg(x,y)=avg(y,x) twice, or\n",
    "computing corr((x,y), (z,w))=corr((w,z), (y,x))=corr((w,z), (x,y))=... multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "edAZvMk7cRLR",
    "outputId": "be3bae8d-d159-40f9-b6ae-115e46f2f37b"
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").config(\"spark.driver.memory\", \"25g\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p3PXjs6DgMqJ"
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"data_processed.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O-eZdLFKhPI_"
   },
   "outputs": [],
   "source": [
    "temp = []\n",
    "names = []\n",
    "for i in df:\n",
    "  try:\n",
    "    test = spark.createDataFrame(df[i])\n",
    "  except:\n",
    "    names.append(i)\n",
    "\n",
    "for i in names:\n",
    "  del df[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xq2xWm9siE4K"
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "dataframes = []\n",
    "for key in df.keys():\n",
    "  dataframes.append(df[key])\n",
    "\n",
    "new_df = pd.concat(dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ALS2o4WCiISo"
   },
   "outputs": [],
   "source": [
    "# Create Spark dataframe\n",
    "sparkdf = spark.createDataFrame(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wqThw7mKj_tq"
   },
   "outputs": [],
   "source": [
    "clean_df = sparkdf.drop('highest price').drop('lowest price').drop('closing price').drop('volumes')\n",
    "dataframe_sp = clean_df.groupBy(\"name\").agg(F.collect_list(\"opening price\").alias(\"op. price\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "knFOjxHWuna1"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import SparkContext,SparkConf\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.mllib.stat import Statistics\n",
    "from pyspark.ml.linalg import SparseVector, DenseVector\n",
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "conf = SparkConf().setAppName(\"miniProject\").setMaster(\"local[*]\")\n",
    "sc = SparkContext.getOrCreate(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sy7bYmT5whX8"
   },
   "outputs": [],
   "source": [
    "# Create subset\n",
    "subset_dataframe = spark.createDataFrame(dataframe_sp.take(100))\n",
    "pandas_subset = dataframe_sp.take(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "HkKIQW__Ea1T",
    "outputId": "13942d3b-9335-49c9-ac8f-dfecd7083b42"
   },
   "outputs": [],
   "source": [
    "tuple_list = []\n",
    "for name, value in pandas_subset:\n",
    "    tuple_list.append((name, value))\n",
    "\n",
    "#print(tuple_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JYDCSr--C7zM"
   },
   "outputs": [],
   "source": [
    "from scipy.stats.stats import pearsonr\n",
    "def pearson(data1, data2):\n",
    "  name1_1 = data1[0]\n",
    "  name2_1 = data2[0]\n",
    "  return (pearsonr(data1[1],data2[1]), name1_1 + \" X \" + name2_1)\n",
    "\n",
    "\n",
    "def subsets_eq_k(A,K):\n",
    "    subsets = []\n",
    "    N = len(A)\n",
    "\n",
    "    # iterate over subsets of size K\n",
    "    mask = (1<<K)-1     # 2^K - 1 is always a number having exactly K 1 bits\n",
    "    while mask < (1<<N):\n",
    "        subset = []\n",
    "        for n in range(N):\n",
    "            if ((mask>>n)&1) == 1:\n",
    "                subset.append(A[n])\n",
    " \n",
    "        subsets.append(subset)\n",
    " \n",
    "        # catch special case\n",
    "        if mask == 0:\n",
    "            break\n",
    " \n",
    "        # determine next mask with Gosper's hack\n",
    "        a = mask & -mask                # determine rightmost 1 bit\n",
    "        b = mask + a                    # determine carry bit\n",
    "        mask = int(((mask^b)>>2)/a) | b # produce block of ones that begins at the least-significant bit\n",
    "\n",
    "    return subsets\n",
    "\n",
    "def subsets_leq_k(A,K):\n",
    "    #subsets = []\n",
    "    collection_subsets = [[] for i in range(K)]\n",
    "    N = len(A)\n",
    " \n",
    "    # iterate over subsets of size less or equal K\n",
    "    mask = 0\n",
    "    while mask < (1<<N): \n",
    "        subset = []\n",
    "        for n in range(N):\n",
    "            if ((mask>>n)&1) == 1:\n",
    "                subset.append(A[n])\n",
    "        if len(subset) > 0:\n",
    "            collection_subsets[len(subset)-1].append(subset)\n",
    "        #subsets.append(subset)\n",
    " \n",
    "        # catch special case when K is zero\n",
    "        if K == 0:\n",
    "            break\n",
    " \n",
    "        # determine next mask\n",
    "        if bin(mask).count(\"1\") < K:\n",
    "            mask += 1\n",
    "        else:\n",
    "            mask = (mask|(mask-1))+1\n",
    "\n",
    "    return collection_subsets \n",
    " \n",
    "def calculate_average(x):\n",
    "    length = len(x)\n",
    "    t = np.zeros(len(x[0][1]))\n",
    "    names = []\n",
    "    for i in range(length):\n",
    "        t = np.add(t,x[i][1])\n",
    "        names.append(x[i][0])\n",
    "    t = t/length\n",
    "    final_name = \"\"\n",
    "    for name in names:\n",
    "      if final_name == \"\":\n",
    "        final_name = final_name + name\n",
    "      else:\n",
    "        final_name = final_name + \"->\" + name\n",
    "    return [[(final_name,t.tolist())]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 494
    },
    "colab_type": "code",
    "id": "HRraPi5pHGg9",
    "outputId": "8b527513-aa51-4fda-f9aa-a493b8f61c7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairs created....moving on....\n",
      "We have arrived at the reduce part of this assignment, best regards. Me, Stefan,Arngrimur\n",
      "With partitioning -->  6.367847919464111\n",
      "[((0.9016482345221795, 1.1775833790978404e-131), 'Forex_XOF->Madrid_ENG X London_DRX'), ((0.9292854748367462, 4.684697827159139e-156), 'Forex_EURBOB->Paris_ML X Mailand_ISP'), ((0.9223248244638435, 4.489713890439076e-149), 'London_SMDS->Forex_EURKPW X Mailand_ISP'), ((-0.9225926956136744, 2.487777333736805e-149), 'Forex_XOF->London_CBG X Mailand_ISP'), ((0.9396734244942876, 6.295204815769106e-168), 'Forex_EURBOB->Sydney_PRT X Mailand_ISP'), ((0.9329655310447126, 4.8398337544414604e-160), 'NYSE_DFS X Xetra_555200->NYSE-American_VTI-IV'), ((0.907435127349094, 4.124012501904595e-136), 'NYSE_DFS->Paris_ML X Sydney_MOC'), ((0.907891859460439, 1.7833342563938065e-136), 'Sydney_MOC X Xetra_A0Z2ZZ->London_SMDS'), ((0.9265316965406868, 3.271962008923153e-153), 'NYSE_NUE->Tokio_6471 X Sydney_MOC'), ((0.9238009318659833, 1.689142737215196e-150), 'Sydney_MOC X Xetra_A0Z2ZZ->London_AGK')]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import time\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "import functools \n",
    "\n",
    "\n",
    "array_dict = {}\n",
    "\n",
    "def reduce_mapping(x):\n",
    "  def pearson(data1, data2):\n",
    "    name1_1 = data1[1]\n",
    "    name2_1 = data2[1]\n",
    "    \n",
    "    value1 = array_dict[data1[0]][0]\n",
    "    value2 = array_dict[data1[0]][1]\n",
    "    \n",
    "    return (pearsonr(value1,value2), name1_1 + \" X \" + name2_1)\n",
    "  return [functools.reduce(pearson, group) for _, group in groupby(sorted(x), key=itemgetter(0))]\n",
    "\n",
    "def milestone_2(aggregation, correlation, p, data, partition):\n",
    "\n",
    "    #instances = 1#int(conf.get(\"spark.executor.instances\"))\n",
    "    #cores = int(conf.get(\"spark.executor.cores\"))\n",
    "    #total_cores = instances * cores\n",
    "    #partitions = total_cores * 2\n",
    "\n",
    "    subsets = subsets_leq_k(data,p-1)\n",
    "    pair_averages = [subsets[0]]\n",
    "    for i in range(1, len(subsets)):\n",
    "        temp = sc.parallelize(spark.createDataFrame(subsets[i]).rdd.flatMap(lambda x: (aggregation(x))).collect()).collect()\n",
    "        pair_averages.append(temp)\n",
    "\n",
    "    #partition = int(len(subsets[0]) * len(subsets[1]) / 5)\n",
    "    print(\"Pairs created....moving on....\")\n",
    "    t = 0\n",
    "    all_combinations = [[] for i in range(partition)]\n",
    "    for x in range(math.floor(p/2)):\n",
    "      for s in range(p):\n",
    "        length_in_subset_1 = x+1\n",
    "        length_in_subset_2 = s+1\n",
    "        if (length_in_subset_1 + length_in_subset_2) == p:\n",
    "          for a in range(len(pair_averages[x])):\n",
    "            for b in range(len(pair_averages[s])):\n",
    "              names1 = pair_averages[s][b][0][0].split(\"->\")\n",
    "              names2 = pair_averages[x][a][0][0].split(\"->\")\n",
    "              if not any((True for x in names1 if x in names2)):\n",
    "                index = t % partition\n",
    "                array_dict[t] = [pair_averages[s][b][0][1], pair_averages[x][a][0][1]]\n",
    "                tmp1 = (t, pair_averages[s][b][0][0])\n",
    "                tmp2 = (t, pair_averages[x][a][0][0])\n",
    "                all_combinations[index].append(tmp1)\n",
    "                all_combinations[index].append(tmp2)\n",
    "                t=t+1\n",
    "\n",
    "    print(\"We have arrived at the reduce part of this assignment, best regards. Me, Stefan,Arngrimur\")\n",
    "    start = time.time()\n",
    "    #res = sc.parallelize(all_combinations).partitionBy(6, lambda k: int(k)).map(lambda x: reduce_mapping(x)).filter(lambda line: abs(line[0]) >= threshold).collect()#.collect()\n",
    "    res = sc.parallelize(all_combinations)\n",
    "    res = res.flatMap(lambda x: reduce_mapping(x)).filter(lambda line: abs(line[0][0]) >= 0.9).take(10)#.filter(lambda line: abs(line[0][0]) >= 0.7).reduce( lambda x,y: x + y )#.collect()\n",
    "    end = time.time()\n",
    "    print(\"With partitioning --> \", end-start)\n",
    "    print(res)\n",
    "\n",
    "milestone_2(calculate_average, pearson, 3, tuple_list, 100)\n",
    "#new_dataframe = sc.parallelize(subset_dataframe.rdd.zipWithIndex().flatMap(lambda x: create_index_pair(x, stored_dataframe)).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Milestone2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
