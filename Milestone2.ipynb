{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nDuHs04-5UCU"
   },
   "source": [
    "# Milestone 2 - multiple correlations\n",
    "\n",
    "Find correlations between at least p >= 3 vectors over a threshold t.\n",
    "\n",
    "*   free to choose value of p\n",
    "*   start with p = 3\n",
    "\n",
    "As correlation measures, you should consider the following two:\n",
    "\n",
    "*   total correlation\n",
    "*   pearson correlation, where the vectors of the two sides are computed by linear combinations (e.g. averaged). For example, consider the multiple correlation of vectors representing\n",
    "Rabobank and ING (denoted as R and I, respectively), with the vector representing Microsoft\n",
    "(denoted as M). The multiple correlation is then defined as Corr(M, R+I/2) \n",
    "\n",
    "You also need to define an aggregation function. The functionality of the aggregation function is\n",
    "(possibly) to reduce the number of input time series, e.g., by averaging as in the previous\n",
    "example. Other aggregation functions that might be meaningful include maximum, minimum, the\n",
    "identity function, variance, etc.\n",
    "\n",
    "In particular:\n",
    "- Build and implement an architecture that gets as input your dataset(s), a correlation function,\n",
    "and a suitable aggregation function, and finds the sets with high multiple correlations (over a\n",
    "threshold t). The value of t depends on the dataset and the correlation/aggregation function.\n",
    "Set a value such that you return around 10 results. Your code should not consider solutions\n",
    "where a vector appears more than once in the input parameters (e.g., in both in1 and in2).\n",
    "- Implement the two described correlation functions with two meaningful aggregation functions,\n",
    "and test your code with these. For testing, you can use the same dataset used in Milestone 1,\n",
    "or a non-negligible subset of it.\n",
    "\n",
    "Constraints:\n",
    "1. You should define aggregation and correlation functions separately.\n",
    "2. The aggregation function should implement one of the following generic interfaces (you can\n",
    "slightly modify it if you have a good reason):\n",
    "dataType aggrFunction(List<dataType> in), e.g., for averaging\n",
    "List<dataType> aggrFunction(List<dataType> in), e.g., the identity function\n",
    "3. The correlation function should implement one of the following generic interfaces (you can\n",
    "slightly modify it if you have a good reason):\n",
    "double correlationFunction(aggFunction(List<dataType> in))\n",
    "double correlationFunction(aggFunction(List<dataType> in1),\n",
    "aggFunction(List<dataType> in2))\n",
    "4. The Spark code should take the correlation and aggregation function (any implementations\n",
    "that satisfy the described interface) as input parameters.\n",
    "5. Make sure that you write Spark code that scales out and avoids bottlenecks.\n",
    "Hint: Think how you can efficiently enumerate all possible combinations and assign ranges of 2\n",
    "comparisons to the individual workers.\n",
    "6. When the correlation/aggregation functions are commutative and associative, you should\n",
    "avoid redundant computations. This includes, e.g., computing avg(x,y)=avg(y,x) twice, or\n",
    "computing corr((x,y), (z,w))=corr((w,z), (y,x))=corr((w,z), (x,y))=... multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "edAZvMk7cRLR",
    "outputId": "be3bae8d-d159-40f9-b6ae-115e46f2f37b"
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").config(\"spark.driver.memory\", \"12g\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p3PXjs6DgMqJ"
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"data_processed.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O-eZdLFKhPI_"
   },
   "outputs": [],
   "source": [
    "temp = []\n",
    "names = []\n",
    "for i in df:\n",
    "  try:\n",
    "    test = spark.createDataFrame(df[i])\n",
    "  except:\n",
    "    names.append(i)\n",
    "\n",
    "for i in names:\n",
    "  del df[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xq2xWm9siE4K"
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "dataframes = []\n",
    "for key in df.keys():\n",
    "  dataframes.append(df[key])\n",
    "\n",
    "new_df = pd.concat(dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ALS2o4WCiISo"
   },
   "outputs": [],
   "source": [
    "# Create Spark dataframe\n",
    "sparkdf = spark.createDataFrame(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wqThw7mKj_tq"
   },
   "outputs": [],
   "source": [
    "clean_df = sparkdf.drop('highest price').drop('lowest price').drop('closing price').drop('volumes')\n",
    "dataframe_sp = clean_df.groupBy(\"name\").agg(F.collect_list(\"opening price\").alias(\"op. price\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "knFOjxHWuna1"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import SparkContext,SparkConf\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.mllib.stat import Statistics\n",
    "from pyspark.ml.linalg import SparseVector, DenseVector\n",
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "conf = SparkConf().setAppName(\"miniProject\").setMaster(\"local[*]\")\n",
    "sc = SparkContext.getOrCreate(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sy7bYmT5whX8"
   },
   "outputs": [],
   "source": [
    "# Create subset\n",
    "subset_dataframe = spark.createDataFrame(dataframe_sp.take(200))\n",
    "pandas_subset = dataframe_sp.take(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "HkKIQW__Ea1T",
    "outputId": "13942d3b-9335-49c9-ac8f-dfecd7083b42"
   },
   "outputs": [],
   "source": [
    "tuple_list = []\n",
    "for name, value in pandas_subset:\n",
    "    tuple_list.append((name, value))\n",
    "\n",
    "#print(tuple_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JYDCSr--C7zM"
   },
   "outputs": [],
   "source": [
    "from scipy.stats.stats import pearsonr\n",
    "def pearson(data1, data2):\n",
    "  name1_1 = data1[0]\n",
    "  name2_1 = data2[0]\n",
    "  return (pearsonr(data1[1],data2[1]), name1_1 + \" X \" + name2_1)\n",
    "\n",
    "\n",
    "def subsets_eq_k(A,K):\n",
    "    subsets = []\n",
    "    N = len(A)\n",
    "\n",
    "    # iterate over subsets of size K\n",
    "    mask = (1<<K)-1     # 2^K - 1 is always a number having exactly K 1 bits\n",
    "    while mask < (1<<N):\n",
    "        subset = []\n",
    "        for n in range(N):\n",
    "            if ((mask>>n)&1) == 1:\n",
    "                subset.append(A[n])\n",
    " \n",
    "        subsets.append(subset)\n",
    " \n",
    "        # catch special case\n",
    "        if mask == 0:\n",
    "            break\n",
    " \n",
    "        # determine next mask with Gosper's hack\n",
    "        a = mask & -mask                # determine rightmost 1 bit\n",
    "        b = mask + a                    # determine carry bit\n",
    "        mask = int(((mask^b)>>2)/a) | b # produce block of ones that begins at the least-significant bit\n",
    "\n",
    "    return subsets\n",
    "\n",
    "def subsets_leq_k(A,K):\n",
    "    #subsets = []\n",
    "    collection_subsets = [[] for i in range(K)]\n",
    "    N = len(A)\n",
    " \n",
    "    # iterate over subsets of size less or equal K\n",
    "    mask = 0\n",
    "    while mask < (1<<N): \n",
    "        subset = []\n",
    "        for n in range(N):\n",
    "            if ((mask>>n)&1) == 1:\n",
    "                subset.append(A[n])\n",
    "        if len(subset) > 0:\n",
    "            collection_subsets[len(subset)-1].append(subset)\n",
    "        #subsets.append(subset)\n",
    " \n",
    "        # catch special case when K is zero\n",
    "        if K == 0:\n",
    "            break\n",
    " \n",
    "        # determine next mask\n",
    "        if bin(mask).count(\"1\") < K:\n",
    "            mask += 1\n",
    "        else:\n",
    "            mask = (mask|(mask-1))+1\n",
    "\n",
    "    return collection_subsets \n",
    " \n",
    "def calculate_average(x):\n",
    "    length = len(x)\n",
    "    t = np.zeros(len(x[0][1]))\n",
    "    names = []\n",
    "    for i in range(length):\n",
    "        t = np.add(t,x[i][1])\n",
    "        names.append(x[i][0])\n",
    "    t = t/length\n",
    "    final_name = \"\"\n",
    "    for name in names:\n",
    "      if final_name == \"\":\n",
    "        final_name = final_name + name\n",
    "      else:\n",
    "        final_name = final_name + \"->\" + name\n",
    "    return [[(final_name,t.tolist())]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 494
    },
    "colab_type": "code",
    "id": "HRraPi5pHGg9",
    "outputId": "8b527513-aa51-4fda-f9aa-a493b8f61c7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairs created....moving on....\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n",
      "1300000\n",
      "1400000\n",
      "1500000\n",
      "1600000\n",
      "1700000\n",
      "1800000\n",
      "1900000\n",
      "2000000\n",
      "2100000\n",
      "2200000\n",
      "2300000\n",
      "2400000\n",
      "2500000\n",
      "2600000\n",
      "2700000\n",
      "2800000\n",
      "2900000\n",
      "3000000\n",
      "3100000\n",
      "3200000\n",
      "3300000\n",
      "3400000\n",
      "3500000\n",
      "3600000\n",
      "3700000\n",
      "3800000\n",
      "3900000\n",
      "We have arrived at the reduce part of this assignment, best regards. Me, Stefan,Arngrimur\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.readRDDFromFile.\n: java.lang.OutOfMemoryError: Java heap space\n\tat org.apache.spark.api.python.PythonRDD$.readRDDFromInputStream(PythonRDD.scala:188)\n\tat org.apache.spark.api.python.PythonRDD$.readRDDFromFile(PythonRDD.scala:175)\n\tat org.apache.spark.api.python.PythonRDD.readRDDFromFile(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-42dc04f46c66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m \u001b[0mmilestone_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalculate_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpearson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;31m#new_dataframe = sc.parallelize(subset_dataframe.rdd.zipWithIndex().flatMap(lambda x: create_index_pair(x, stored_dataframe)).collect())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-42dc04f46c66>\u001b[0m in \u001b[0;36mmilestone_2\u001b[0;34m(aggregation, correlation, p, data)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;31m#res = sc.parallelize(all_combinations).partitionBy(6, lambda k: int(k)).map(lambda x: reduce_mapping(x)).filter(lambda line: abs(line[0]) >= threshold).collect()#.collect()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_combinations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mreduce_mapping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0.7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#.filter(lambda line: abs(line[0][0]) >= 0.7).reduce( lambda x,y: x + y )#.collect()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/context.py\u001b[0m in \u001b[0;36mparallelize\u001b[0;34m(self, c, numSlices)\u001b[0m\n\u001b[1;32m    525\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonParallelizeServer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumSlices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m         \u001b[0mjrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_serialize_to_jvm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreader_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreateRDDServer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    528\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/context.py\u001b[0m in \u001b[0;36m_serialize_to_jvm\u001b[0;34m(self, data, serializer, reader_func, createRDDServer)\u001b[0m\n\u001b[1;32m    560\u001b[0m                 \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m                     \u001b[0mtempFile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreader_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtempFile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m                 \u001b[0;31m# we eagerily reads the file so we can delete right after.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/context.py\u001b[0m in \u001b[0;36mreader_func\u001b[0;34m(temp_filename)\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mreader_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 522\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadRDDFromFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumSlices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcreateRDDServer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.readRDDFromFile.\n: java.lang.OutOfMemoryError: Java heap space\n\tat org.apache.spark.api.python.PythonRDD$.readRDDFromInputStream(PythonRDD.scala:188)\n\tat org.apache.spark.api.python.PythonRDD$.readRDDFromFile(PythonRDD.scala:175)\n\tat org.apache.spark.api.python.PythonRDD.readRDDFromFile(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import time\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "import functools \n",
    "\n",
    "def reduce_mapping(x):\n",
    "  def pearson(data1, data2):\n",
    "    name1_1 = data1[1][0]\n",
    "    name2_1 = data2[1][0]\n",
    "    return (pearsonr(data1[1][1],data2[1][1]), name1_1 + \" X \" + name2_1)\n",
    "  \n",
    "  return [functools.reduce(pearson, group) for _, group in groupby(sorted(x), key=itemgetter(0))]\n",
    "\n",
    "def milestone_2(aggregation, correlation, p, data):\n",
    "\n",
    "    #instances = 1#int(conf.get(\"spark.executor.instances\"))\n",
    "    #cores = int(conf.get(\"spark.executor.cores\"))\n",
    "    #total_cores = instances * cores\n",
    "    #partitions = total_cores * 2\n",
    "\n",
    "    subsets = subsets_leq_k(data,p-1)\n",
    "    pair_averages = [subsets[0]]\n",
    "    for i in range(1, len(subsets)):\n",
    "        temp = sc.parallelize(spark.createDataFrame(subsets[i]).rdd.flatMap(lambda x: (aggregation(x))).collect()).collect()\n",
    "        pair_averages.append(temp)\n",
    "\n",
    "    partition = int(len(subsets[0]) * len(subsets[1]) / 5)\n",
    "    print(\"Pairs created....moving on....\")\n",
    "    t = 0\n",
    "    all_combinations = [[] for i in range(partition)]\n",
    "    for x in range(math.floor(p/2)):\n",
    "      for s in range(p):\n",
    "        length_in_subset_1 = x+1\n",
    "        length_in_subset_2 = s+1\n",
    "        if (length_in_subset_1 + length_in_subset_2) == p:\n",
    "          for a in range(len(pair_averages[x])):\n",
    "            for b in range(len(pair_averages[s])):\n",
    "              names1 = pair_averages[s][b][0][0].split(\"->\")\n",
    "              names2 = pair_averages[x][a][0][0].split(\"->\")\n",
    "              if not any((True for x in names1 if x in names2)):\n",
    "                index = t % partition\n",
    "                tmp1 = (t, (pair_averages[s][b][0][0], pair_averages[s][b][0][1]))\n",
    "                tmp2 = (t, (pair_averages[x][a][0][0], pair_averages[x][a][0][1]))\n",
    "                all_combinations[index].append(tmp1)\n",
    "                all_combinations[index].append(tmp2)\n",
    "                if t % 100000 == 0:\n",
    "                  print(t)\n",
    "                t=t+1\n",
    "\n",
    "    print(\"We have arrived at the reduce part of this assignment, best regards. Me, Stefan,Arngrimur\")\n",
    "    start = time.time()\n",
    "    #res = sc.parallelize(all_combinations).partitionBy(6, lambda k: int(k)).map(lambda x: reduce_mapping(x)).filter(lambda line: abs(line[0]) >= threshold).collect()#.collect()\n",
    "    res = sc.parallelize(all_combinations)\n",
    "    res = res.flatMap(lambda x: reduce_mapping(x)).filter(lambda line: abs(line[0][0]) >= 0.7).collect()#.filter(lambda line: abs(line[0][0]) >= 0.7).reduce( lambda x,y: x + y )#.collect()\n",
    "    end = time.time()\n",
    "    print(\"With partitioning --> \", end-start)\n",
    "    print(res[:5])\n",
    "      \n",
    "milestone_2(calculate_average, pearson, 3, tuple_list)\n",
    "#new_dataframe = sc.parallelize(subset_dataframe.rdd.zipWithIndex().flatMap(lambda x: create_index_pair(x, stored_dataframe)).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "OPA4eGIdCo03",
    "outputId": "0df6940a-2fce-4e7a-e263-b68e58dd5046"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "def milestone_2(aggregation, correlation, p, data):\n",
    "    A = [4,5,7,9]\n",
    "    K = p-1\n",
    "\n",
    "\n",
    "    subsets = subsets_leq_k(data,p-1)\n",
    "    pair_averages = [subsets[0]]\n",
    "    for i in range(1, len(subsets)):\n",
    "        temp = sc.parallelize(spark.createDataFrame(subsets[i]).rdd.flatMap(lambda x: (aggregation(x))).collect()).collect()\n",
    "        pair_averages.append(temp)\n",
    "\n",
    "    print(\"Pairs created....moving on....\")\n",
    "    t = 0\n",
    "    all_combinations = []\n",
    "    for x in range(math.floor(p/2)):\n",
    "      for s in range(p):\n",
    "        length_in_subset_1 = x+1\n",
    "        length_in_subset_2 = s+1\n",
    "        if (length_in_subset_1 + length_in_subset_2) == p:\n",
    "          for a in range(len(pair_averages[x])):\n",
    "            for b in range(len(pair_averages[s])):\n",
    "              names1 = pair_averages[s][b][0][0].split(\"->\")\n",
    "              names2 = pair_averages[x][a][0][0].split(\"->\")\n",
    "              if not any((True for x in names1 if x in names2)):\n",
    "                tmp1 = (t, (pair_averages[s][b][0][0], pair_averages[s][b][0][1]))\n",
    "                tmp2 = (t, (pair_averages[x][a][0][0], pair_averages[x][a][0][1]))\n",
    "                all_combinations.append(tmp1)\n",
    "                all_combinations.append(tmp2)\n",
    "                if t % 100000 == 0:\n",
    "                  print(t)\n",
    "                t=t+1\n",
    "    print(t)\n",
    "    print(\"We have arrived at the reduce part of this assignment, best regards. Me, Stefan, Arngrimur\")\n",
    "    start = time.time()\n",
    "    res = sc.parallelize(all_combinations).reduceByKey(correlation).collect()\n",
    "    end = time.time()\n",
    "    print(\"Took --> \", end-start)\n",
    "    print(res[:5])\n",
    "      \n",
    "milestone_2(calculate_average, pearson, 3, tuple_list)\n",
    "\n",
    "#new_dataframe = sc.parallelize(subset_dataframe.rdd.zipWithIndex().flatMap(lambda x: create_index_pair(x, stored_dataframe)).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Milestone2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
